{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER2 One Scanner - Aperio FDA\n",
    "\n",
    "- 3-Fold (50/50) split, No Holdout Set\n",
    "- Truth = Categorical from Mean of 7 continuous scores \n",
    "- Epoch at automatic Stop when loss<.001 change \n",
    "- LeNet model, 10 layers, Dropout (0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras.callbacks import EarlyStopping\n",
    "from PIL import Image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Lambda\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "import csv\n",
    "import cv2\n",
    "import scipy\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For single scanner\n",
    "BASE_PATH = '/home/diam/Desktop/HER2_data_categorical/Aperio_FDA/'\n",
    "#BASE PATH for working from home:\n",
    "#BASE_PATH = '/home/OSEL/Desktop/HER2_data_categorical/'\n",
    "#epochs = 10\n",
    "batch_size = 32\n",
    "num_classes = 3\n",
    "#epochs = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data - Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the version from Ravi's code:\n",
    "\n",
    "#FDA\n",
    "#X_FDA = []\n",
    "#idx_FDA = []\n",
    "#for index, image_filename in list(enumerate(BASE_PATH)):\n",
    "#\timg_file = cv2.imread(BASE_PATH + '/' + image_filename)\n",
    "#\tif img_file is not None:\n",
    "\t\t#img_file = smisc.imresize(arr = img_file, size = (600,760,3))\n",
    "#\t\timg_file = smisc.imresize(arr = img_file, size = (120,160,3))\t\t\n",
    "#\t\timg_arr = np.asarray(img_file)\n",
    "#\t\tX_FDA.append(img_arr)\n",
    "#\t\tidx_FDA.append(index)\n",
    "\n",
    "#X_FDA = np.asarray(X_FDA)\n",
    "#idx_FDA = np.asarray(idx_FDA)\n",
    "#random.seed(rs)\n",
    "#random_id = random.sample(idx_FDA, len(idx_FDA)/2)\n",
    "#random_FDA = []\n",
    "#for i in random_id:\n",
    "#\trandom_FDA.append(X_FDA[i])\n",
    "\n",
    "#random_FDA = np.asarray(random_FDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data - Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder):\n",
    "    X = []\n",
    "    y = []\n",
    "    filenames = []\n",
    "\n",
    "    for hclass in os.listdir(folder):\n",
    "        if not hclass.startswith('.'):\n",
    "            if hclass in [\"1\"]:\n",
    "                label = 1\n",
    "            else: #label must be 1 or 2\n",
    "                if hclass in [\"2\"]:\n",
    "                    label = 2\n",
    "                else:\n",
    "                    label = 3\n",
    "            for image_filename in os.listdir(folder + hclass):\n",
    "                filename = folder + hclass + '/' + image_filename\n",
    "                img_file = cv2.imread(folder + hclass + '/' + image_filename)\n",
    "                \n",
    "                if img_file is not None:\n",
    "                    img_file = scipy.misc.imresize(arr=img_file, size=(120, 160, 3))\n",
    "                    img_arr = np.asarray(img_file)\n",
    "                    X.append(img_arr)\n",
    "                    y.append(label)\n",
    "                    filenames.append(filename)\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    z = np.asarray(filenames)\n",
    "    return X,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n",
      "241\n",
      "241\n"
     ]
    }
   ],
   "source": [
    "X, y, z = get_data(BASE_PATH)\n",
    "\n",
    "\n",
    "#print(X)\n",
    "#print(y)\n",
    "#print(z)\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "print(len(z))\n",
    "\n",
    "#INTEGER ENCODE\n",
    "#https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "encoder = LabelEncoder()\n",
    "y_cat = np_utils.to_categorical(encoder.fit_transform(y))\n",
    "#print(y_cat)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#encoder = LabelEncoder()\n",
    "#encoder.fit(y)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "#encoded_y_train = encoder.transform(y_train)\n",
    "#encoded_y_test = encoder.transform(y_test)\n",
    "\n",
    "#y_train = np_utils.to_categorical(encoded_y_train)\n",
    "#y_test = np_utils.to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model with K-Fold X-Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "KFold(n_splits=2, random_state=5, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "#Need to remove random_state before running multiple thousands of times and averaging\n",
    "kf = KFold(n_splits = 2, random_state=5, shuffle=True)\n",
    "print(kf.get_n_splits(y))\n",
    "print(kf)\n",
    "\n",
    "\n",
    "for train_index_full, test_index in kf.split(y):\n",
    "    val_index = np.random.choice(train_index_full, 21, replace=False)\n",
    "    train_index = [x for x in train_index_full if x not in val_index]\n",
    "    X_train, X_val, X_test = X[train_index], X[val_index], X[test_index]\n",
    "    y_train, y_val, y_test = y[train_index], y[val_index], y[test_index]\n",
    "    z_train, z_val, z_test = z[train_index], z[val_index], z[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_index)\n",
    "#len(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold #1\n",
      "(12.0, <type 'numpy.float64'>)\n",
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/1000\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 1.7085 - acc: 0.4259 - val_loss: 1.2404 - val_acc: 0.1667\n",
      "Epoch 2/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.9388 - acc: 0.5741 - val_loss: 1.2037 - val_acc: 0.1667\n",
      "Epoch 3/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.1955 - acc: 0.4630 - val_loss: 0.8708 - val_acc: 0.9167\n",
      "Epoch 4/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.9081 - acc: 0.6296 - val_loss: 1.0612 - val_acc: 0.1667\n",
      "Epoch 5/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.9375 - acc: 0.6111 - val_loss: 0.5679 - val_acc: 0.8333\n",
      "Epoch 6/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.9986 - acc: 0.5648 - val_loss: 0.6468 - val_acc: 0.9167\n",
      "Epoch 7/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.8502 - acc: 0.7037 - val_loss: 0.6084 - val_acc: 0.9167\n",
      "Epoch 8/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.7176 - acc: 0.7685 - val_loss: 0.4301 - val_acc: 0.8333\n",
      "Epoch 9/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.5093 - acc: 0.8148 - val_loss: 0.9138 - val_acc: 0.7500\n",
      "Epoch 10/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.1146 - acc: 0.7222 - val_loss: 0.4842 - val_acc: 0.8333\n",
      "Epoch 11/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.7220 - acc: 0.8056 - val_loss: 0.6526 - val_acc: 0.7500\n",
      "Epoch 12/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.9034 - acc: 0.6667 - val_loss: 0.4652 - val_acc: 0.9167\n",
      "Epoch 13/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.7125 - acc: 0.7315 - val_loss: 0.4817 - val_acc: 0.9167\n",
      "Epoch 14/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5477 - acc: 0.8519 - val_loss: 0.4913 - val_acc: 0.8333\n",
      "Epoch 15/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4937 - acc: 0.8519 - val_loss: 0.4575 - val_acc: 0.8333\n",
      "Epoch 16/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.8020 - acc: 0.7130 - val_loss: 0.5069 - val_acc: 0.9167\n",
      "Epoch 17/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5155 - acc: 0.8333 - val_loss: 0.5434 - val_acc: 0.8333\n",
      "Epoch 18/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5210 - acc: 0.8519 - val_loss: 0.3479 - val_acc: 0.9167\n",
      "Epoch 19/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.8170 - acc: 0.7778 - val_loss: 0.4694 - val_acc: 0.9167\n",
      "Epoch 20/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5857 - acc: 0.8241 - val_loss: 0.4409 - val_acc: 0.8333\n",
      "Epoch 21/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4777 - acc: 0.8519 - val_loss: 0.5115 - val_acc: 0.8333\n",
      "Epoch 22/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4443 - acc: 0.8611 - val_loss: 1.1430 - val_acc: 0.5000\n",
      "Epoch 23/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5568 - acc: 0.8333 - val_loss: 0.4408 - val_acc: 0.8333\n",
      "Epoch 24/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4240 - acc: 0.8796 - val_loss: 0.3476 - val_acc: 0.9167\n",
      "Epoch 25/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4428 - acc: 0.8611 - val_loss: 0.5897 - val_acc: 0.7500\n",
      "Epoch 26/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.7300 - acc: 0.7685 - val_loss: 0.3611 - val_acc: 0.9167\n",
      "Epoch 27/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.1028 - acc: 0.6852 - val_loss: 0.4149 - val_acc: 0.9167\n",
      "Epoch 28/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4672 - acc: 0.8704 - val_loss: 0.6322 - val_acc: 0.8333\n",
      "Epoch 29/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.4255 - acc: 0.8611 - val_loss: 0.3183 - val_acc: 0.9167\n",
      "Epoch 30/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.6537 - acc: 0.7870 - val_loss: 0.3882 - val_acc: 0.9167\n",
      "Epoch 31/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.4884 - acc: 0.8333 - val_loss: 0.5606 - val_acc: 0.8333\n",
      "Epoch 32/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4253 - acc: 0.8796 - val_loss: 0.8525 - val_acc: 0.7500\n",
      "Epoch 33/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.5954 - acc: 0.7870 - val_loss: 0.4341 - val_acc: 0.8333\n",
      "Epoch 34/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.6122 - acc: 0.8148 - val_loss: 0.3605 - val_acc: 0.9167\n",
      "Epoch 35/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.5820 - acc: 0.7870 - val_loss: 0.3509 - val_acc: 0.9167\n",
      "Epoch 36/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4840 - acc: 0.8704 - val_loss: 0.3959 - val_acc: 0.8333\n",
      "Epoch 37/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.3726 - acc: 0.8981 - val_loss: 1.0812 - val_acc: 0.6667\n",
      "Epoch 38/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.9012 - acc: 0.7315 - val_loss: 0.4680 - val_acc: 0.8333\n",
      "Epoch 39/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.4213 - acc: 0.8981 - val_loss: 0.5138 - val_acc: 0.8333\n",
      "Epoch 40/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.3813 - acc: 0.8981 - val_loss: 0.3003 - val_acc: 0.9167\n",
      "Epoch 41/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.4100 - acc: 0.8981 - val_loss: 0.3132 - val_acc: 0.9167\n",
      "Epoch 42/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4625 - acc: 0.8519 - val_loss: 0.3802 - val_acc: 0.8333\n",
      "Epoch 43/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.4507 - acc: 0.8426 - val_loss: 0.3015 - val_acc: 0.9167\n",
      "Epoch 44/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.4509 - acc: 0.8611 - val_loss: 0.4436 - val_acc: 0.8333\n",
      "Epoch 45/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.4089 - acc: 0.8611 - val_loss: 0.3010 - val_acc: 0.9167\n",
      "Epoch 46/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4495 - acc: 0.8796 - val_loss: 0.8184 - val_acc: 0.7500\n",
      "Epoch 47/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4774 - acc: 0.8426 - val_loss: 0.5972 - val_acc: 0.8333\n",
      "Epoch 48/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.3568 - acc: 0.8981 - val_loss: 0.3229 - val_acc: 0.9167\n",
      "Epoch 49/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.3758 - acc: 0.8796 - val_loss: 0.2789 - val_acc: 0.9167\n",
      "Epoch 50/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.3781 - acc: 0.8889 - val_loss: 0.2882 - val_acc: 0.9167\n",
      "Epoch 51/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.5542 - acc: 0.8426 - val_loss: 0.3863 - val_acc: 0.9167\n",
      "Epoch 52/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.3669 - acc: 0.8889 - val_loss: 0.3955 - val_acc: 0.8333\n",
      "Epoch 53/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.3285 - acc: 0.8981 - val_loss: 0.2789 - val_acc: 0.9167\n",
      "Epoch 54/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.3023 - acc: 0.8981 - val_loss: 0.3848 - val_acc: 0.8333\n",
      "Epoch 55/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.3480 - acc: 0.9074 - val_loss: 0.6901 - val_acc: 0.8333\n",
      "Epoch 56/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4648 - acc: 0.8519 - val_loss: 0.5335 - val_acc: 0.8333\n",
      "Epoch 57/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.3195 - acc: 0.9074 - val_loss: 0.5665 - val_acc: 0.8333\n",
      "Epoch 58/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4670 - acc: 0.8519 - val_loss: 0.6366 - val_acc: 0.8333\n",
      "Epoch 59/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.3354 - acc: 0.9167 - val_loss: 0.3826 - val_acc: 0.9167\n",
      "Epoch 60/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.3982 - acc: 0.8796 - val_loss: 1.7743 - val_acc: 0.5000\n",
      "Epoch 61/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 2ms/step - loss: 0.7635 - acc: 0.7315 - val_loss: 0.4107 - val_acc: 0.8333\n",
      "Epoch 62/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.3103 - acc: 0.9074 - val_loss: 0.2952 - val_acc: 0.9167\n",
      "Epoch 63/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.3210 - acc: 0.8981 - val_loss: 0.4354 - val_acc: 0.8333\n",
      "Epoch 64/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2617 - acc: 0.9167 - val_loss: 0.3204 - val_acc: 0.8333\n",
      "Epoch 65/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.3105 - acc: 0.9074 - val_loss: 0.4206 - val_acc: 0.8333\n",
      "Epoch 66/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.3054 - acc: 0.9074 - val_loss: 0.2575 - val_acc: 0.9167\n",
      "Epoch 67/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.3090 - acc: 0.8889 - val_loss: 0.3495 - val_acc: 0.8333\n",
      "Epoch 68/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.5373 - acc: 0.8426 - val_loss: 0.4929 - val_acc: 0.8333\n",
      "Epoch 69/1000\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.3279 - acc: 0.885 - 0s 2ms/step - loss: 0.3079 - acc: 0.8981 - val_loss: 0.4525 - val_acc: 0.8333\n",
      "Epoch 70/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.2535 - acc: 0.9167 - val_loss: 0.2590 - val_acc: 0.9167\n",
      "Epoch 71/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2414 - acc: 0.9074 - val_loss: 0.6220 - val_acc: 0.8333\n",
      "Epoch 72/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2321 - acc: 0.9167 - val_loss: 0.6788 - val_acc: 0.8333\n",
      "Epoch 73/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.2422 - acc: 0.9074 - val_loss: 0.5102 - val_acc: 0.8333\n",
      "Epoch 74/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2321 - acc: 0.8981 - val_loss: 4.5827 - val_acc: 0.1667\n",
      "Epoch 75/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 2.0723 - acc: 0.6204 - val_loss: 0.3536 - val_acc: 0.9167\n",
      "Epoch 76/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.3885 - acc: 0.8889 - val_loss: 0.3360 - val_acc: 0.9167\n",
      "Epoch 77/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2786 - acc: 0.9167 - val_loss: 0.3202 - val_acc: 0.8333\n",
      "Epoch 78/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2068 - acc: 0.9074 - val_loss: 0.2713 - val_acc: 0.9167\n",
      "Epoch 79/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2248 - acc: 0.9167 - val_loss: 0.5117 - val_acc: 0.8333\n",
      "Epoch 80/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.2588 - acc: 0.9167 - val_loss: 0.4103 - val_acc: 0.8333\n",
      "Epoch 81/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2324 - acc: 0.9074 - val_loss: 0.2831 - val_acc: 0.9167\n",
      "Epoch 82/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2527 - acc: 0.9074 - val_loss: 0.3706 - val_acc: 0.8333\n",
      "Epoch 83/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2475 - acc: 0.9074 - val_loss: 0.5539 - val_acc: 0.8333\n",
      "Epoch 84/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2234 - acc: 0.8981 - val_loss: 0.3996 - val_acc: 0.8333\n",
      "Epoch 85/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2248 - acc: 0.9167 - val_loss: 1.0714 - val_acc: 0.7500\n",
      "Epoch 86/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.4381 - acc: 0.8889 - val_loss: 0.3643 - val_acc: 0.8333\n",
      "Epoch 87/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2379 - acc: 0.9074 - val_loss: 0.2632 - val_acc: 0.9167\n",
      "Epoch 88/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2978 - acc: 0.8889 - val_loss: 0.2505 - val_acc: 0.9167\n",
      "Epoch 89/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.4669 - acc: 0.8148 - val_loss: 0.2538 - val_acc: 0.9167\n",
      "Epoch 90/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2238 - acc: 0.9167 - val_loss: 0.4073 - val_acc: 0.8333\n",
      "Epoch 91/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.2164 - acc: 0.8981 - val_loss: 0.3928 - val_acc: 0.8333\n",
      "Epoch 92/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1839 - acc: 0.9167 - val_loss: 0.6699 - val_acc: 0.8333\n",
      "Epoch 93/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1559 - acc: 0.9167 - val_loss: 0.2607 - val_acc: 0.8333\n",
      "Epoch 94/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1696 - acc: 0.9167 - val_loss: 0.4985 - val_acc: 0.8333\n",
      "Epoch 95/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1839 - acc: 0.9167 - val_loss: 0.9013 - val_acc: 0.7500\n",
      "Epoch 96/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1686 - acc: 0.9167 - val_loss: 0.4814 - val_acc: 0.8333\n",
      "Epoch 97/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.3184 - acc: 0.8981 - val_loss: 0.3081 - val_acc: 0.9167\n",
      "Epoch 98/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2719 - acc: 0.8981 - val_loss: 0.4571 - val_acc: 0.8333\n",
      "Epoch 99/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2217 - acc: 0.9074 - val_loss: 0.6145 - val_acc: 0.8333\n",
      "Epoch 100/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2672 - acc: 0.8981 - val_loss: 0.2263 - val_acc: 0.9167\n",
      "Epoch 101/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1715 - acc: 0.9074 - val_loss: 0.3711 - val_acc: 0.8333\n",
      "Epoch 102/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.3266 - acc: 0.8796 - val_loss: 1.2925 - val_acc: 0.5833\n",
      "Epoch 103/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.2690 - acc: 0.8796 - val_loss: 0.2542 - val_acc: 0.8333\n",
      "Epoch 104/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1760 - acc: 0.9167 - val_loss: 0.4650 - val_acc: 0.8333\n",
      "Epoch 105/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1512 - acc: 0.9259 - val_loss: 0.2828 - val_acc: 0.8333\n",
      "Epoch 106/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2017 - acc: 0.9074 - val_loss: 0.6714 - val_acc: 0.8333\n",
      "Epoch 107/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1274 - acc: 0.9167 - val_loss: 0.1973 - val_acc: 0.9167\n",
      "Epoch 108/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1704 - acc: 0.9444 - val_loss: 0.2892 - val_acc: 0.9167\n",
      "Epoch 109/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1382 - acc: 0.9259 - val_loss: 0.3493 - val_acc: 0.8333\n",
      "Epoch 110/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1265 - acc: 0.9259 - val_loss: 0.8224 - val_acc: 0.7500\n",
      "Epoch 111/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1503 - acc: 0.9352 - val_loss: 0.5528 - val_acc: 0.8333\n",
      "Epoch 112/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.5448 - acc: 0.8519 - val_loss: 0.2763 - val_acc: 0.9167\n",
      "Epoch 113/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.3009 - acc: 0.9074 - val_loss: 0.2179 - val_acc: 0.9167\n",
      "Epoch 114/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1829 - acc: 0.9352 - val_loss: 0.3928 - val_acc: 0.8333\n",
      "Epoch 115/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1100 - acc: 0.9444 - val_loss: 0.4338 - val_acc: 0.8333\n",
      "Epoch 116/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0849 - acc: 0.9630 - val_loss: 1.0987 - val_acc: 0.7500\n",
      "Epoch 117/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1075 - acc: 0.9352 - val_loss: 0.8508 - val_acc: 0.8333\n",
      "Epoch 118/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1165 - acc: 0.9352 - val_loss: 0.2215 - val_acc: 0.8333\n",
      "Epoch 119/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1323 - acc: 0.9352 - val_loss: 0.3962 - val_acc: 0.8333\n",
      "Epoch 120/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4501 - acc: 0.8889 - val_loss: 1.1368 - val_acc: 0.7500\n",
      "Epoch 121/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 1ms/step - loss: 0.2780 - acc: 0.9167 - val_loss: 0.2375 - val_acc: 0.8333\n",
      "Epoch 122/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1515 - acc: 0.9167 - val_loss: 0.3013 - val_acc: 0.8333\n",
      "Epoch 123/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.1151 - acc: 0.9352 - val_loss: 0.3687 - val_acc: 0.8333\n",
      "Epoch 124/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0976 - acc: 0.9537 - val_loss: 0.6122 - val_acc: 0.8333\n",
      "Epoch 125/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.0892 - acc: 0.9537 - val_loss: 0.3209 - val_acc: 0.8333\n",
      "Epoch 126/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0882 - acc: 0.9722 - val_loss: 0.2789 - val_acc: 0.8333\n",
      "Epoch 127/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.0765 - acc: 0.9722 - val_loss: 0.4980 - val_acc: 0.8333\n",
      "Epoch 128/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.2179 - acc: 0.9537 - val_loss: 0.6877 - val_acc: 0.8333\n",
      "Epoch 129/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1109 - acc: 0.9537 - val_loss: 0.6870 - val_acc: 0.8333\n",
      "Epoch 130/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.2965 - acc: 0.9074 - val_loss: 0.1850 - val_acc: 0.9167\n",
      "Epoch 131/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.1578 - acc: 0.9167 - val_loss: 0.4158 - val_acc: 0.8333\n",
      "Epoch 132/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0861 - acc: 0.9815 - val_loss: 0.1981 - val_acc: 0.9167\n",
      "Epoch 133/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.1302 - acc: 0.9259 - val_loss: 1.7294 - val_acc: 0.6667\n",
      "Epoch 134/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1621 - acc: 0.9352 - val_loss: 2.5508 - val_acc: 0.4167\n",
      "Epoch 135/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.4527 - acc: 0.8148 - val_loss: 0.2535 - val_acc: 0.9167\n",
      "Epoch 136/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1467 - acc: 0.9722 - val_loss: 0.1753 - val_acc: 0.9167\n",
      "Epoch 137/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.1365 - acc: 0.9352 - val_loss: 0.3298 - val_acc: 0.8333\n",
      "Epoch 138/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1065 - acc: 0.9630 - val_loss: 0.2153 - val_acc: 0.9167\n",
      "Epoch 139/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.1205 - acc: 0.9537 - val_loss: 0.1865 - val_acc: 0.9167\n",
      "Epoch 140/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.0977 - acc: 0.9722 - val_loss: 0.2975 - val_acc: 0.8333\n",
      "Epoch 141/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.1119 - acc: 0.9352 - val_loss: 0.4276 - val_acc: 0.8333\n",
      "Epoch 142/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1203 - acc: 0.9259 - val_loss: 5.5331 - val_acc: 0.1667\n",
      "Epoch 143/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.8390 - acc: 0.8519 - val_loss: 0.3368 - val_acc: 0.9167\n",
      "Epoch 144/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.1309 - acc: 0.9537 - val_loss: 0.2010 - val_acc: 0.9167\n",
      "Epoch 145/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1347 - acc: 0.9259 - val_loss: 0.1619 - val_acc: 0.9167\n",
      "Epoch 146/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0781 - acc: 0.9630 - val_loss: 0.2090 - val_acc: 0.9167\n",
      "Epoch 147/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.0726 - acc: 0.9537 - val_loss: 0.1948 - val_acc: 0.9167\n",
      "Epoch 148/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.0876 - acc: 0.9537 - val_loss: 0.2213 - val_acc: 0.8333\n",
      "Epoch 149/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0665 - acc: 0.9537 - val_loss: 0.3479 - val_acc: 0.8333\n",
      "Epoch 150/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.0484 - acc: 0.9815 - val_loss: 0.2198 - val_acc: 0.9167\n",
      "Epoch 151/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.3452 - acc: 0.9444 - val_loss: 0.1872 - val_acc: 0.9167\n",
      "Epoch 152/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.4415 - acc: 0.8889 - val_loss: 0.2354 - val_acc: 0.9167\n",
      "Epoch 153/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0536 - acc: 0.9907 - val_loss: 0.2748 - val_acc: 0.8333\n",
      "Epoch 154/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0673 - acc: 0.9722 - val_loss: 0.5790 - val_acc: 0.8333\n",
      "Epoch 155/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0972 - acc: 0.9630 - val_loss: 0.2452 - val_acc: 0.9167\n",
      "Epoch 156/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.0614 - acc: 0.9815 - val_loss: 0.3554 - val_acc: 0.8333\n",
      "Epoch 157/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0348 - acc: 0.9907 - val_loss: 0.2979 - val_acc: 0.8333\n",
      "Epoch 158/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.0461 - acc: 0.9815 - val_loss: 0.3335 - val_acc: 0.9167\n",
      "Epoch 159/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0497 - acc: 0.9815 - val_loss: 0.6042 - val_acc: 0.8333\n",
      "Epoch 160/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0530 - acc: 0.9815 - val_loss: 0.8069 - val_acc: 0.8333\n",
      "Epoch 161/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.0432 - acc: 0.9722 - val_loss: 0.4360 - val_acc: 0.9167\n",
      "Epoch 162/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.9349 - acc: 0.6852 - val_loss: 0.1986 - val_acc: 0.9167\n",
      "Epoch 163/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.1020 - acc: 0.9537 - val_loss: 0.1760 - val_acc: 0.9167\n",
      "Epoch 164/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.1263 - acc: 0.9630 - val_loss: 0.2033 - val_acc: 0.9167\n",
      "Epoch 165/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.0983 - acc: 0.9537 - val_loss: 0.2848 - val_acc: 0.9167\n",
      "Epoch 166/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0647 - acc: 0.9815 - val_loss: 0.2651 - val_acc: 0.9167\n",
      "Epoch 167/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.0497 - acc: 0.9815 - val_loss: 0.3368 - val_acc: 0.9167\n",
      "Epoch 168/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.1999 - val_acc: 0.9167\n",
      "Epoch 169/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.0491 - acc: 0.9907 - val_loss: 0.2553 - val_acc: 0.8333\n",
      "Epoch 170/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 0.0559 - acc: 0.9722 - val_loss: 0.2228 - val_acc: 0.9167\n",
      "Epoch 00170: early stopping\n",
      "Fold Score (accuracy): 0.793388429752\n",
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "[[  3.59891865e-27   1.69281277e-12   1.00000000e+00]\n",
      " [  3.17518537e-28   5.85775243e-13   1.00000000e+00]\n",
      " [  5.39149623e-05   9.98942554e-01   1.00347889e-03]\n",
      " [  1.81600861e-02   9.81125593e-01   7.14377093e-04]\n",
      " [  4.50610399e-01   5.47930837e-01   1.45873649e-03]\n",
      " [  3.50246234e-30   5.10453401e-06   9.99994874e-01]\n",
      " [  4.49135550e-04   9.99485493e-01   6.53315437e-05]\n",
      " [  4.76415595e-03   9.94994342e-01   2.41491492e-04]\n",
      " [  1.03432834e-02   9.87339616e-01   2.31717993e-03]\n",
      " [  4.99199640e-13   9.99999166e-01   8.66899825e-07]\n",
      " [  2.15658045e-04   8.93696666e-01   1.06087662e-01]\n",
      " [  1.26401097e-01   8.70298028e-01   3.30083328e-03]\n",
      " [  7.80520093e-11   9.99995828e-01   4.21850655e-06]\n",
      " [  6.87524537e-10   9.99999642e-01   3.69849545e-07]\n",
      " [  3.25204858e-26   3.96301360e-11   1.00000000e+00]\n",
      " [  2.05371761e-14   9.52058599e-07   9.99999046e-01]\n",
      " [  1.77775455e-05   3.65662396e-01   6.34319782e-01]\n",
      " [  3.37844976e-04   9.99364913e-01   2.97296327e-04]\n",
      " [  1.31619765e-06   1.27867227e-02   9.87212002e-01]\n",
      " [  1.99466842e-04   9.81198549e-01   1.86020471e-02]\n",
      " [  2.68596230e-26   4.37174429e-12   1.00000000e+00]\n",
      " [  6.06700704e-02   9.36701536e-01   2.62845983e-03]\n",
      " [  1.04333091e-16   4.87558793e-07   9.99999523e-01]\n",
      " [  8.67013961e-10   2.02854746e-03   9.97971475e-01]\n",
      " [  5.98456687e-15   1.18807555e-04   9.99881148e-01]\n",
      " [  1.23190454e-29   3.20734953e-12   1.00000000e+00]\n",
      " [  1.13958349e-05   9.98614907e-01   1.37362443e-03]\n",
      " [  6.33788203e-33   6.00430157e-16   1.00000000e+00]\n",
      " [  1.10541748e-07   9.99999285e-01   5.49173194e-07]\n",
      " [  2.07528683e-05   9.96083617e-01   3.89562245e-03]\n",
      " [  3.26212447e-34   3.60515010e-16   1.00000000e+00]\n",
      " [  2.28822470e-01   7.69059479e-01   2.11805920e-03]\n",
      " [  2.31972386e-33   3.69310812e-16   1.00000000e+00]\n",
      " [  2.21062007e-16   3.11172869e-07   9.99999642e-01]\n",
      " [  9.65411799e-19   1.60451673e-05   9.99983907e-01]\n",
      " [  8.06950735e-22   6.52555741e-07   9.99999404e-01]\n",
      " [  2.99777123e-24   6.23665486e-12   1.00000000e+00]\n",
      " [  2.98015424e-04   9.99328852e-01   3.73153976e-04]\n",
      " [  1.02948384e-16   4.05288301e-03   9.95947182e-01]\n",
      " [  4.15880641e-04   9.99336541e-01   2.47500458e-04]\n",
      " [  5.01429252e-02   9.48466778e-01   1.39026612e-03]\n",
      " [  8.09990466e-02   9.05259669e-01   1.37413107e-02]\n",
      " [  1.52636825e-09   6.80193901e-01   3.19806099e-01]\n",
      " [  1.65796466e-02   9.80338335e-01   3.08192638e-03]\n",
      " [  4.71122207e-18   9.99991655e-01   8.36426989e-06]\n",
      " [  2.42742522e-34   2.91050029e-15   1.00000000e+00]\n",
      " [  1.38048454e-13   4.94970736e-05   9.99950528e-01]\n",
      " [  2.18711330e-06   9.99207318e-01   7.90578139e-04]\n",
      " [  6.65385420e-35   8.04607979e-17   1.00000000e+00]\n",
      " [  8.28081816e-02   9.15751636e-01   1.44020747e-03]\n",
      " [  1.88627709e-02   9.80641901e-01   4.95365704e-04]\n",
      " [  3.77024293e-07   9.99999404e-01   2.83761125e-07]\n",
      " [  3.92907439e-03   9.95949268e-01   1.21621932e-04]\n",
      " [  8.22618771e-30   2.11717173e-13   1.00000000e+00]\n",
      " [  6.11526266e-21   2.00968330e-07   9.99999762e-01]\n",
      " [  3.40326585e-22   8.00360278e-10   1.00000000e+00]\n",
      " [  9.63295224e-06   5.15090704e-01   4.84899610e-01]\n",
      " [  4.17554349e-01   5.62859654e-01   1.95859857e-02]\n",
      " [  6.28235983e-03   7.65206873e-01   2.28510767e-01]\n",
      " [  1.22129903e-04   7.42078304e-01   2.57799536e-01]\n",
      " [  1.07247750e-07   9.00918484e-01   9.90814194e-02]\n",
      " [  3.04965228e-02   9.68730032e-01   7.73464737e-04]\n",
      " [  4.49457586e-17   1.31571582e-07   9.99999881e-01]\n",
      " [  4.00514114e-34   1.46866407e-16   1.00000000e+00]\n",
      " [  5.56583416e-07   9.99998569e-01   8.83183361e-07]\n",
      " [  1.91071862e-03   9.95125353e-01   2.96394317e-03]\n",
      " [  2.76232779e-01   7.22055554e-01   1.71168649e-03]\n",
      " [  1.29662482e-02   9.86859620e-01   1.74123707e-04]\n",
      " [  1.67130893e-05   9.99975920e-01   7.43680721e-06]\n",
      " [  1.06682748e-07   9.99997497e-01   2.42604665e-06]\n",
      " [  8.87303646e-16   1.15690391e-05   9.99988437e-01]\n",
      " [  6.26416886e-06   2.78012007e-01   7.21981704e-01]\n",
      " [  3.56863331e-22   6.68268385e-10   1.00000000e+00]\n",
      " [  1.14711216e-02   9.87610579e-01   9.18360194e-04]\n",
      " [  1.22041823e-02   9.87496734e-01   2.99129257e-04]\n",
      " [  8.21725465e-03   9.91353214e-01   4.29485634e-04]\n",
      " [  7.82210050e-23   1.00000000e+00   1.22628188e-12]\n",
      " [  7.45485067e-07   9.41895485e-01   5.81036955e-02]\n",
      " [  3.13864425e-02   9.68140244e-01   4.73295804e-04]\n",
      " [  2.56609321e-02   9.53855872e-01   2.04832666e-02]\n",
      " [  8.82443269e-38   1.34939892e-16   1.00000000e+00]\n",
      " [  4.03040461e-03   9.95734155e-01   2.35461412e-04]\n",
      " [  3.32149565e-02   9.59557056e-01   7.22795259e-03]\n",
      " [  3.76661122e-01   6.20213211e-01   3.12565453e-03]\n",
      " [  1.47375322e-04   9.99022841e-01   8.29829834e-04]\n",
      " [  8.03451869e-04   9.99125779e-01   7.07344589e-05]\n",
      " [  1.16409440e-22   1.04582965e-09   1.00000000e+00]\n",
      " [  3.61480484e-10   9.54046904e-04   9.99045908e-01]\n",
      " [  3.03839945e-04   9.98542905e-01   1.15321833e-03]\n",
      " [  8.69441469e-16   7.86787950e-06   9.99992132e-01]\n",
      " [  3.42700705e-02   9.64994371e-01   7.35532958e-04]\n",
      " [  2.81258370e-04   9.99244332e-01   4.74481407e-04]\n",
      " [  9.54093957e-06   7.68803716e-01   2.31186703e-01]\n",
      " [  9.89830085e-08   6.77381158e-02   9.32261765e-01]\n",
      " [  4.18965995e-01   5.78121781e-01   2.91225919e-03]\n",
      " [  1.98834416e-04   8.35386872e-01   1.64414257e-01]\n",
      " [  7.21685529e-01   2.76606083e-01   1.70833699e-03]\n",
      " [  3.23187860e-14   1.39383785e-02   9.86061692e-01]\n",
      " [  4.48613372e-16   8.25862545e-10   1.00000000e+00]\n",
      " [  4.40244924e-11   5.14467189e-04   9.99485493e-01]\n",
      " [  1.66581064e-13   1.14612376e-04   9.99885321e-01]\n",
      " [  9.57960378e-10   9.46693957e-01   5.33060804e-02]\n",
      " [  1.04978026e-36   1.64481854e-17   1.00000000e+00]\n",
      " [  1.09014815e-18   8.50015525e-09   1.00000000e+00]\n",
      " [  1.07703246e-02   7.78603196e-01   2.10626438e-01]\n",
      " [  2.95568459e-08   9.99275386e-01   7.24591955e-04]\n",
      " [  3.03047132e-02   9.68323946e-01   1.37128914e-03]\n",
      " [  3.61766145e-02   9.62692201e-01   1.13123073e-03]\n",
      " [  5.70790178e-07   9.91184652e-01   8.81475676e-03]\n",
      " [  7.13516147e-06   9.99912858e-01   7.99445916e-05]\n",
      " [  1.31071161e-07   9.99998212e-01   1.63481138e-06]\n",
      " [  4.32498084e-04   9.07035887e-01   9.25315917e-02]\n",
      " [  3.69911553e-14   9.99987841e-01   1.21603698e-05]\n",
      " [  2.42514551e-01   7.52372622e-01   5.11276489e-03]\n",
      " [  6.57388557e-17   1.55952748e-07   9.99999881e-01]\n",
      " [  9.41545295e-04   9.98930037e-01   1.28383967e-04]\n",
      " [  5.24469937e-16   4.77519279e-07   9.99999523e-01]\n",
      " [  1.64276736e-15   1.34184035e-02   9.86581564e-01]\n",
      " [  2.11137021e-07   9.99969006e-01   3.07992595e-05]\n",
      " [  1.89891405e-04   9.99798954e-01   1.12518019e-05]\n",
      " [  1.01459709e-10   5.44583490e-05   9.99945521e-01]]\n",
      "[2 2 1 1 1 2 1 1 1 1 1 1 1 1 2 2 2 1 2 1 2 1 2 2 2 2 1 2 1 1 2 1 2 2 2 2 2\n",
      " 1 2 1 1 1 1 1 1 2 2 1 2 1 1 1 1 2 2 2 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 2 2 1\n",
      " 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 2 1 1 1 2 1 1 0 2 2 2 2 1 2 2 1 1 1 1 1 1 1\n",
      " 1 1 1 2 1 2 2 1 1 2]\n",
      "fold #2\n",
      "(12.0, <type 'numpy.float64'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 109 samples, validate on 12 samples\n",
      "Epoch 1/1000\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 1.5897 - acc: 0.3761 - val_loss: 1.0453 - val_acc: 0.2500\n",
      "Epoch 2/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 1.0754 - acc: 0.4128 - val_loss: 0.9795 - val_acc: 0.6667\n",
      "Epoch 3/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.9995 - acc: 0.5138 - val_loss: 0.8803 - val_acc: 0.6667\n",
      "Epoch 4/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.9267 - acc: 0.5046 - val_loss: 0.8618 - val_acc: 0.6667\n",
      "Epoch 5/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.8161 - acc: 0.6789 - val_loss: 9.2927 - val_acc: 0.2500\n",
      "Epoch 6/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 2.7744 - acc: 0.6239 - val_loss: 0.7428 - val_acc: 0.7500\n",
      "Epoch 7/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.6571 - acc: 0.8073 - val_loss: 0.5544 - val_acc: 0.9167\n",
      "Epoch 8/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 1.2267 - acc: 0.6239 - val_loss: 0.7766 - val_acc: 0.6667\n",
      "Epoch 9/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.8089 - acc: 0.6881 - val_loss: 0.5661 - val_acc: 0.9167\n",
      "Epoch 10/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.6656 - acc: 0.8073 - val_loss: 0.7900 - val_acc: 0.6667\n",
      "Epoch 11/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.8048 - acc: 0.6606 - val_loss: 0.5940 - val_acc: 0.8333\n",
      "Epoch 12/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.6147 - acc: 0.7982 - val_loss: 0.5507 - val_acc: 0.8333\n",
      "Epoch 13/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.5974 - acc: 0.7798 - val_loss: 0.4719 - val_acc: 0.8333\n",
      "Epoch 14/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.5346 - acc: 0.8165 - val_loss: 2.1914 - val_acc: 0.4167\n",
      "Epoch 15/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 1.1700 - acc: 0.5963 - val_loss: 0.6093 - val_acc: 0.8333\n",
      "Epoch 16/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.6721 - acc: 0.7798 - val_loss: 0.5486 - val_acc: 0.8333\n",
      "Epoch 17/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.6296 - acc: 0.7798 - val_loss: 0.5380 - val_acc: 0.8333\n",
      "Epoch 18/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.6830 - acc: 0.7615 - val_loss: 0.5766 - val_acc: 0.9167\n",
      "Epoch 19/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.5618 - acc: 0.8165 - val_loss: 0.4049 - val_acc: 0.9167\n",
      "Epoch 20/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.6811 - acc: 0.7615 - val_loss: 0.5899 - val_acc: 0.8333\n",
      "Epoch 21/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.5620 - acc: 0.8073 - val_loss: 0.4420 - val_acc: 0.9167\n",
      "Epoch 22/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.4666 - acc: 0.8257 - val_loss: 0.3985 - val_acc: 0.9167\n",
      "Epoch 23/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 1.0710 - acc: 0.6330 - val_loss: 0.7675 - val_acc: 0.6667\n",
      "Epoch 24/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.8167 - acc: 0.6972 - val_loss: 0.5241 - val_acc: 0.8333\n",
      "Epoch 25/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.5634 - acc: 0.8257 - val_loss: 0.7421 - val_acc: 0.7500\n",
      "Epoch 26/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.7761 - acc: 0.6697 - val_loss: 0.5253 - val_acc: 0.8333\n",
      "Epoch 27/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.4809 - acc: 0.8440 - val_loss: 0.3743 - val_acc: 0.9167\n",
      "Epoch 28/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.5322 - acc: 0.8165 - val_loss: 0.7539 - val_acc: 0.7500\n",
      "Epoch 29/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.6186 - acc: 0.7706 - val_loss: 0.3868 - val_acc: 0.9167\n",
      "Epoch 30/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.4340 - acc: 0.8349 - val_loss: 0.3548 - val_acc: 0.9167\n",
      "Epoch 31/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.5411 - acc: 0.8257 - val_loss: 0.8241 - val_acc: 0.6667\n",
      "Epoch 32/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.6263 - acc: 0.7706 - val_loss: 0.8011 - val_acc: 0.6667\n",
      "Epoch 33/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.6672 - acc: 0.7890 - val_loss: 0.7257 - val_acc: 0.7500\n",
      "Epoch 34/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.5440 - acc: 0.8440 - val_loss: 0.3504 - val_acc: 0.9167\n",
      "Epoch 35/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.4025 - acc: 0.8440 - val_loss: 0.4732 - val_acc: 0.8333\n",
      "Epoch 36/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.8666 - acc: 0.6972 - val_loss: 0.4903 - val_acc: 0.8333\n",
      "Epoch 37/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.4533 - acc: 0.8440 - val_loss: 0.3808 - val_acc: 0.9167\n",
      "Epoch 38/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.6508 - acc: 0.7615 - val_loss: 0.4000 - val_acc: 0.9167\n",
      "Epoch 39/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.4299 - acc: 0.8716 - val_loss: 0.3489 - val_acc: 0.9167\n",
      "Epoch 40/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.3593 - acc: 0.8624 - val_loss: 0.5479 - val_acc: 0.8333\n",
      "Epoch 41/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.4655 - acc: 0.8073 - val_loss: 0.4165 - val_acc: 0.9167\n",
      "Epoch 42/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.4027 - acc: 0.8257 - val_loss: 2.0728 - val_acc: 0.5000\n",
      "Epoch 43/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.5780 - acc: 0.7615 - val_loss: 0.3691 - val_acc: 0.9167\n",
      "Epoch 44/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.3172 - acc: 0.8807 - val_loss: 0.4632 - val_acc: 0.9167\n",
      "Epoch 45/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.5522 - acc: 0.7890 - val_loss: 1.0884 - val_acc: 0.6667\n",
      "Epoch 46/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.4575 - acc: 0.8257 - val_loss: 0.3521 - val_acc: 0.9167\n",
      "Epoch 47/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.4148 - acc: 0.8624 - val_loss: 0.3786 - val_acc: 0.9167\n",
      "Epoch 48/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.3185 - acc: 0.8991 - val_loss: 0.7461 - val_acc: 0.7500\n",
      "Epoch 49/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.3555 - acc: 0.8624 - val_loss: 0.8043 - val_acc: 0.6667\n",
      "Epoch 50/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.5249 - acc: 0.7798 - val_loss: 0.3894 - val_acc: 0.9167\n",
      "Epoch 51/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.3050 - acc: 0.8991 - val_loss: 0.6158 - val_acc: 0.7500\n",
      "Epoch 52/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.4280 - acc: 0.8257 - val_loss: 1.2507 - val_acc: 0.6667\n",
      "Epoch 53/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.7355 - acc: 0.7982 - val_loss: 0.4511 - val_acc: 0.9167\n",
      "Epoch 54/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.3246 - acc: 0.8716 - val_loss: 0.4740 - val_acc: 0.8333\n",
      "Epoch 55/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.3692 - acc: 0.8716 - val_loss: 0.6310 - val_acc: 0.7500\n",
      "Epoch 56/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.4654 - acc: 0.8349 - val_loss: 0.5595 - val_acc: 0.9167\n",
      "Epoch 57/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.3175 - acc: 0.9083 - val_loss: 0.6686 - val_acc: 0.9167\n",
      "Epoch 58/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.2793 - acc: 0.8807 - val_loss: 0.3531 - val_acc: 0.9167\n",
      "Epoch 59/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.2900 - acc: 0.8807 - val_loss: 0.5838 - val_acc: 0.9167\n",
      "Epoch 60/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.4852 - acc: 0.8349 - val_loss: 0.9054 - val_acc: 0.6667\n",
      "Epoch 61/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 1ms/step - loss: 0.5945 - acc: 0.7706 - val_loss: 0.3747 - val_acc: 0.9167\n",
      "Epoch 62/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.3254 - acc: 0.8899 - val_loss: 0.5778 - val_acc: 0.9167\n",
      "Epoch 63/1000\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.3298 - acc: 0.8899 - val_loss: 0.4069 - val_acc: 0.9167\n",
      "Epoch 64/1000\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.2865 - acc: 0.8991 - val_loss: 0.3526 - val_acc: 0.9167\n",
      "Epoch 00064: early stopping\n",
      "Fold Score (accuracy): 0.825\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "[[  3.98870485e-11   6.28595114e-01   3.71404886e-01]\n",
      " [  2.67878681e-01   6.90002501e-01   4.21188883e-02]\n",
      " [  1.29042419e-05   9.44808483e-01   5.51784895e-02]\n",
      " [  6.32530595e-14   2.77315289e-01   7.22684741e-01]\n",
      " [  2.23607823e-01   7.43739605e-01   3.26526091e-02]\n",
      " [  3.45554911e-02   9.23551500e-01   4.18929942e-02]\n",
      " [  4.25968915e-02   9.04318511e-01   5.30845784e-02]\n",
      " [  5.35075041e-03   9.50577080e-01   4.40721326e-02]\n",
      " [  1.20729395e-13   1.29239634e-01   8.70760441e-01]\n",
      " [  2.91842610e-01   6.73151493e-01   3.50058824e-02]\n",
      " [  2.69346256e-09   7.31651068e-01   2.68348962e-01]\n",
      " [  2.33035088e-01   7.36746311e-01   3.02186068e-02]\n",
      " [  2.92036414e-01   6.81132495e-01   2.68310588e-02]\n",
      " [  2.26957127e-01   7.47329950e-01   2.57129353e-02]\n",
      " [  2.86196053e-01   6.75555050e-01   3.82489748e-02]\n",
      " [  1.12043694e-01   8.61959159e-01   2.59971563e-02]\n",
      " [  5.95647357e-02   9.27660167e-01   1.27750784e-02]\n",
      " [  1.51625854e-05   9.57636356e-01   4.23484817e-02]\n",
      " [  5.53264804e-02   9.30714786e-01   1.39586991e-02]\n",
      " [  2.63604313e-01   6.91205740e-01   4.51899692e-02]\n",
      " [  2.67339498e-01   7.07206547e-01   2.54539587e-02]\n",
      " [  4.97576545e-15   3.67911428e-01   6.32088602e-01]\n",
      " [  1.09412500e-10   5.28507054e-01   4.71492976e-01]\n",
      " [  1.83365890e-03   9.70088899e-01   2.80774124e-02]\n",
      " [  2.23673210e-02   8.95956337e-01   8.16763788e-02]\n",
      " [  2.38322601e-01   7.21498132e-01   4.01792563e-02]\n",
      " [  1.58344342e-24   5.60330926e-04   9.99439657e-01]\n",
      " [  3.83231930e-21   1.12611553e-04   9.99887347e-01]\n",
      " [  1.22831389e-01   8.15397978e-01   6.17706552e-02]\n",
      " [  5.49826073e-04   5.22577167e-01   4.76872981e-01]\n",
      " [  2.01471925e-01   7.69252539e-01   2.92755403e-02]\n",
      " [  1.87533796e-02   9.66746688e-01   1.44998720e-02]\n",
      " [  5.91061400e-28   1.44895253e-04   9.99855042e-01]\n",
      " [  1.10143446e-04   9.85200524e-01   1.46893449e-02]\n",
      " [  1.35400978e-14   1.25415802e-01   8.74584138e-01]\n",
      " [  1.73049141e-02   9.47778225e-01   3.49168070e-02]\n",
      " [  1.62248984e-01   7.98460066e-01   3.92909646e-02]\n",
      " [  2.87464142e-01   6.84453428e-01   2.80824490e-02]\n",
      " [  1.43413052e-01   8.15541267e-01   4.10456099e-02]\n",
      " [  2.92910099e-01   6.68987513e-01   3.81023325e-02]\n",
      " [  2.88382560e-01   6.83912754e-01   2.77047269e-02]\n",
      " [  3.03987712e-01   6.59285545e-01   3.67268175e-02]\n",
      " [  2.26362720e-01   7.27674484e-01   4.59628478e-02]\n",
      " [  3.10559243e-01   6.59416258e-01   3.00244950e-02]\n",
      " [  3.11739713e-01   6.60098553e-01   2.81616785e-02]\n",
      " [  2.83937318e-22   7.28218025e-03   9.92717862e-01]\n",
      " [  6.63622168e-23   1.52923423e-03   9.98470724e-01]\n",
      " [  4.37206472e-06   9.63744998e-01   3.62506136e-02]\n",
      " [  2.62591869e-01   7.16131687e-01   2.12764647e-02]\n",
      " [  4.37059850e-12   4.66421902e-01   5.33578038e-01]\n",
      " [  1.35677806e-32   1.62239630e-05   9.99983788e-01]\n",
      " [  5.40669820e-09   9.72483218e-01   2.75168195e-02]\n",
      " [  7.52396204e-24   8.46483856e-02   9.15351629e-01]\n",
      " [  1.15793891e-01   8.57107401e-01   2.70987190e-02]\n",
      " [  5.34247747e-03   6.00246608e-01   3.94410878e-01]\n",
      " [  2.49312283e-03   9.08060253e-01   8.94467086e-02]\n",
      " [  2.95039177e-01   6.74324989e-01   3.06358505e-02]\n",
      " [  2.34081000e-01   7.21427798e-01   4.44911867e-02]\n",
      " [  1.08683765e-01   8.70430291e-01   2.08859611e-02]\n",
      " [  1.40485907e-29   1.96899334e-03   9.98030961e-01]\n",
      " [  9.71691726e-23   2.00818013e-03   9.97991800e-01]\n",
      " [  3.12507331e-01   6.56281471e-01   3.12111583e-02]\n",
      " [  1.66777030e-01   7.92566121e-01   4.06567566e-02]\n",
      " [  4.34543153e-07   6.23815358e-01   3.76184195e-01]\n",
      " [  5.10386471e-03   9.84111547e-01   1.07845757e-02]\n",
      " [  1.60118932e-21   9.88776624e-01   1.12233320e-02]\n",
      " [  2.43832171e-01   7.29644358e-01   2.65234746e-02]\n",
      " [  3.57288077e-26   4.74802917e-03   9.95252013e-01]\n",
      " [  5.70948236e-02   7.50959814e-01   1.91945285e-01]\n",
      " [  2.89715737e-01   6.77279711e-01   3.30044813e-02]\n",
      " [  2.01662755e-29   1.00519814e-04   9.99899507e-01]\n",
      " [  5.26189332e-08   5.79401314e-01   4.20598626e-01]\n",
      " [  3.85219278e-03   9.89460468e-01   6.68732775e-03]\n",
      " [  1.48296919e-28   4.55271831e-04   9.99544680e-01]\n",
      " [  9.14646965e-03   9.13745165e-01   7.71083906e-02]\n",
      " [  9.65877407e-05   3.72893989e-01   6.27009451e-01]\n",
      " [  1.18252185e-25   9.99342501e-01   6.57506054e-04]\n",
      " [  0.00000000e+00   7.85893377e-08   9.99999881e-01]\n",
      " [  1.18462704e-05   9.88570988e-01   1.14170834e-02]\n",
      " [  0.00000000e+00   1.44019454e-08   1.00000000e+00]\n",
      " [  2.34074563e-01   7.33464241e-01   3.24611478e-02]\n",
      " [  7.40966843e-10   9.75645661e-01   2.43543908e-02]\n",
      " [  2.87005723e-01   6.84953332e-01   2.80409995e-02]\n",
      " [  7.40230055e-09   6.20412529e-01   3.79587442e-01]\n",
      " [  3.01903456e-01   6.67832196e-01   3.02643161e-02]\n",
      " [  2.79121131e-01   6.91183805e-01   2.96951272e-02]\n",
      " [  1.24116855e-06   3.18619937e-01   6.81378841e-01]\n",
      " [  7.25018904e-02   9.06302869e-01   2.11952347e-02]\n",
      " [  2.33906224e-01   7.39118934e-01   2.69748308e-02]\n",
      " [  8.00622553e-02   7.62266219e-01   1.57671511e-01]\n",
      " [  1.47786131e-30   1.08120625e-03   9.98918772e-01]\n",
      " [  4.34596323e-29   9.42543993e-05   9.99905705e-01]\n",
      " [  7.37016602e-16   7.11900089e-03   9.92880940e-01]\n",
      " [  2.31684938e-01   7.20615089e-01   4.77000326e-02]\n",
      " [  3.08899909e-01   6.62124872e-01   2.89752781e-02]\n",
      " [  3.06359977e-01   6.65952981e-01   2.76870299e-02]\n",
      " [  1.81071321e-03   9.36664939e-01   6.15244396e-02]\n",
      " [  2.36372754e-01   7.36755431e-01   2.68717594e-02]\n",
      " [  3.07803392e-01   6.63846791e-01   2.83497591e-02]\n",
      " [  2.58494556e-01   7.09943712e-01   3.15617770e-02]\n",
      " [  3.90553750e-22   7.80361235e-01   2.19638765e-01]\n",
      " [  2.20456168e-01   7.44887829e-01   3.46560143e-02]\n",
      " [  2.95825303e-01   6.70677185e-01   3.34975272e-02]\n",
      " [  0.00000000e+00   6.23233802e-07   9.99999404e-01]\n",
      " [  1.45091694e-02   9.64388072e-01   2.11027451e-02]\n",
      " [  1.17453425e-27   4.60916664e-04   9.99539137e-01]\n",
      " [  2.93814600e-01   6.77830338e-01   2.83550564e-02]\n",
      " [  1.68874791e-07   4.46464807e-01   5.53534985e-01]\n",
      " [  4.99147281e-24   4.14844928e-03   9.95851517e-01]\n",
      " [  3.04818958e-01   6.67282939e-01   2.78980620e-02]\n",
      " [  2.11234540e-01   7.63555646e-01   2.52097808e-02]\n",
      " [  2.16873474e-02   9.58515108e-01   1.97975226e-02]\n",
      " [  3.49345542e-31   7.06201303e-04   9.99293804e-01]\n",
      " [  5.62144449e-34   2.32044977e-06   9.99997735e-01]\n",
      " [  9.29400166e-29   3.56126286e-04   9.99643922e-01]\n",
      " [  2.27564029e-04   9.93481755e-01   6.29072962e-03]\n",
      " [  2.23576040e-16   4.09736425e-01   5.90263546e-01]\n",
      " [  1.20274816e-13   1.65477186e-01   8.34522784e-01]\n",
      " [  4.63475369e-10   8.66093040e-02   9.13390636e-01]\n",
      " [  2.55290251e-23   2.71865781e-02   9.72813427e-01]]\n",
      "[1 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 2 1 1 1 1 2 1 2 1 1\n",
      " 1 1 1 1 1 1 1 1 2 2 1 1 2 2 1 2 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 1 1 2 1 1 2\n",
      " 1 2 1 2 1 2 1 1 1 1 1 1 2 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 2 1 1\n",
      " 1 2 2 2 1 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "\n",
    "for train_index_full, test_index in kf.split(y_cat):\n",
    "    fold+=1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    \n",
    "    random.shuffle(train_index_full)\n",
    "    random.shuffle(test_index)\n",
    "    \n",
    "    val_number = np.rint(len(train_index_full)*0.1)\n",
    "    print(val_number, type(val_number))\n",
    "    val_index = np.random.choice(train_index_full, int(val_number), replace=False)\n",
    "    train_index = [x for x in train_index_full if x not in val_index]\n",
    "    X_train, X_val, X_test = X[train_index], X[val_index], X[test_index]\n",
    "    y_train, y_val, y_test = y_cat[train_index], y_cat[val_index], y_cat[test_index]\n",
    "    z_train, z_val, z_test = z[train_index], z[val_index], z[test_index]\n",
    "    \n",
    "    #X_train_full = X[train]\n",
    "    #y_train_full = y_cat[train]\n",
    "    #X_test = X[test]\n",
    "    #y_test = y_cat[test]\n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "        fill_mode='constant',\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        )\n",
    "    \n",
    "    #for _ in datagen.flow(y_test):\n",
    "    #    if y_test[]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x * 1./255., input_shape=(120, 160, 3), output_shape=(120, 160, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(120, 160, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='rmsprop',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=25, verbose=1, mode='auto')\n",
    "   \n",
    "    \n",
    "    model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            validation_data=(X_val,y_val),\n",
    "            callbacks=[monitor],\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1,\n",
    "            epochs=1000)\n",
    "        \n",
    "    pred = model.predict(X_test)\n",
    "        \n",
    "    oos_y.append(y_test)\n",
    "    #pred = np.argmax(pred)\n",
    "    predx = np.argmax(pred,axis=1)\n",
    "    oos_pred.append(predx)\n",
    "    \n",
    "    #measure the fold's accuracy\n",
    "    y_compare = np.argmax(y_test,axis=1) #for accuracy calculation\n",
    "    score = metrics.accuracy_score(y_compare, predx)\n",
    "    print(\"Fold Score (accuracy): {}\".format(score))\n",
    "    \n",
    "    #Create confusion matrix\n",
    "    \n",
    "    print(y_test)\n",
    "    #print(test)\n",
    "    print(pred)\n",
    "    print(predx)\n",
    "    #print(z[test])\n",
    "    #print(oos_y)\n",
    "    \n",
    "    \n",
    "    #outlist = []\n",
    "    \n",
    "    #outline = index[0] + '\\t' + str(z[test]) + '\\t' + str(int(pred[0])) + '\\t' + str(int(pred[1])) + '\\t' + str(int(pred[2])) + '\\t' + str(int(y[test])) + '\\t' + str(predx) + \\n'\n",
    "    #outlist.append(outline)\n",
    "    #with open(\"results.txt\", \"w\") as f:\n",
    "        #f.writelines(outlist)\n",
    "        #f.writelines(z[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)\n",
    "boolean = y_train[:,0] == 1\n",
    "print(boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = cv2.imread('/home/diam/Desktop/HER2_data_aug_0/FDA_0_aug/aug_8_7339873.tif')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhsv = cv2.cvtColor(f,cv2.COLOR_BGR2HSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhsv > im.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('imtest.tif', fhsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libtiff import TIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
